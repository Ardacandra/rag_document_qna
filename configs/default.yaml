# Configuration for the RAG pipeline
# LLM_MODEL: "llama3.1:8b"
LLM_MODEL: "llama3.2:3B"
OLLAMA_BASE_URL: "http://localhost:11434"

# Embedding model (Hugging Face local model)
EMBED_MODEL: "BAAI/bge-small-en-v1.5"
EMBED_MODEL_TYPE: "local"  # set to 'local' to prefer local HuggingFace embeddings

# # Optional keys
# COHERE_API_KEY: ""
# OPENAI_API_KEY: ""

# Paths and index settings
DATA_DIR: "knowledge_base"
CHROMA_PATH: "chroma_db"
COLLECTION_NAME: "advanced_rag_collection"
CHUNK_SIZE: 512
CHUNK_OVERLAP: 20

RERANK_TOP_N: 5
RETRIEVE_K: 20